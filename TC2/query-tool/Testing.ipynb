{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0ef4602-83bb-4a7f-b2fd-0334e8bd5b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.9'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import llama_cpp\n",
    "\n",
    "llama_cpp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "589a86f0-7570-4bfe-b32b-5f641de38d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm_chat = Llama(model_path = \"./llama-2-7b.Q4_K_S.gguf\", n_gpu_threads=-1, use_mmap=False, verbose=False, n_ctx=4096, n_threads=8)\n",
    "llm_emb = Llama(model_path = \"./llama-2-7b.Q4_K_S.gguf\", n_gpu_threads=-1, use_mmap=False, verbose=False, n_ctx=4096, n_threads=8, embedding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c813bf68-72bc-4ab6-869d-92e6e5a8f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "txt_path = \"../data/txt\"\n",
    "pdf_path = \"../data/pdf\"\n",
    "\n",
    "pdf_files = [f for f in os.listdir(pdf_path) if os.path.isfile(os.path.join(pdf_path, f))]\n",
    "full_txt_files = [os.path.join(txt_path, f) for f in os.listdir(txt_path) if os.path.isfile(os.path.join(txt_path, f))]\n",
    "\n",
    "txt = [open(f, encoding=\"utf-8\").read() for f in full_txt_files]\n",
    "all_txt = \"[NEW FILE] \".join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ad9e405-c0b4-4e37-a497-7f9561028adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 298 chunks\n"
     ]
    }
   ],
   "source": [
    "def chunk_with_llamacpp(text, chunk_size=512, overlap=64):\n",
    "    b = text.encode(\"utf-8\")\n",
    "    toks = llm_emb.tokenize(b, add_bos=False, special=False)\n",
    "    chunks = []\n",
    "    for i in range(0, len(toks), chunk_size - overlap):\n",
    "        window = toks[i : i + chunk_size]\n",
    "        chunks.append(llm_emb.detokenize(window))\n",
    "        if i + chunk_size >= len(toks):\n",
    "            break\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunks = chunk_with_llamacpp(all_txt)\n",
    "print(f\"Generated {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "776cd245-f5f0-4880-8739-45c01bfd16e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_txt = [c.decode(\"utf-8\", errors=\"ignore\") if isinstance(c, (bytes, bytearray)) else c for c in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c90ba73d-c262-4ded-b8ec-b4107b8ad3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "with open(\"chunks_txt.pkl\", \"wb\") as file:\n",
    "    pickle.dump(chunks_txt, file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1f77253-a551-4624-88e6-892749b084ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(512, 4096)\n",
      "(512, 4096)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "res = llm_emb.create_embedding(chunks_txt[:2])\n",
    "print(len(res[\"data\"]))\n",
    "print(np.array(res[\"data\"][0][\"embedding\"]).shape)\n",
    "print(np.array(res[\"data\"][1][\"embedding\"]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f572cd1-c804-4482-a699-aaedc827dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "emb_list = []\n",
    "for chunk in tqdm(chunks_txt, desc=\"Embedding chunks\"):\n",
    "    resp = llm_emb.create_embedding(input=chunk)\n",
    "    emb = resp[\"data\"][0][\"embedding\"]\n",
    "    emb_list.append(emb)\n",
    "\n",
    "emb_array = np.array(emb_list, dtype=np.float32)\n",
    "np.save(\"embeddings.npy\", emb_array)\n",
    "print(f\"Saved {emb_array.shape[0]} embeddings (dim={emb_array.shape[1]}) to embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06772c06-4aee-4f34-a319-75cfb845fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\"\"\"\n",
    "with open(\"embeddings_list.pkl\", \"wb\") as file:\n",
    "    pickle.dump(emb_list, file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b9cf651-fb18-47eb-9891-65634e876060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "298"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('embeddings_list.pkl', 'rb') as file:\n",
    "    emb_list = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539f82c0-170b-4b0f-9170-aee1321faa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf0b605-9af8-4c89-a459-3442b311bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chunks = len(emb_list)\n",
    "dim = 4096\n",
    "fixed_length = 512\n",
    "pooled_np = []\n",
    "\n",
    "for tok_emb in emb_list:\n",
    "    n, d = len(tok_emb), len(tok_emb[0])\n",
    "    if n < fixed_length:\n",
    "        pad = np.zeros((fixed_length - n, d), dtype=np.float32)\n",
    "        tok_emb = np.vstack([tok_emb, pad])\n",
    "    elif n > fixed_length:\n",
    "        tok_emb = tok_emb[:fixed_length]\n",
    "    pooled_np.append(np.array(tok_emb,dtype=np.float32).mean(axis=0))\n",
    "\n",
    "# 3) stack into one array of shape (n_chunks, 512, dim)\n",
    "pooled_np = np.stack(pooled_np, axis=0)\n",
    "print(\"pooled_np.shape =\", pooled_np.shape)  # â†’ (298, 512, 4096)\n",
    "\n",
    "\"\"\"\n",
    "np.save(\"pooled_np.npy\", pooled_np)\n",
    "print(\"Saved raw token embeddings to pooled_np.npy\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2be140f-088d-455a-b657-9f2ffa360277",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_np = np.load(\"pooled_np.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3427a70-54a1-4201-9fbc-9ecfba04f24d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eece050d-1f21-4840-9670-cd29f2c9d532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ”ï¸  Indexed 298 chunk embeddings (dim=4096)\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "pooled_np = np.load(\"pooled_np.npy\")\n",
    "with open(\"chunks.pkl\", \"rb\") as f:\n",
    "    chunks_txt = pickle.load(f)\n",
    "\n",
    "n_chunks, dim = pooled_np.shape\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(pooled_np.astype(np.float32)) \n",
    "\n",
    "print(f\"âœ”ï¸  Indexed {index.ntotal} chunk embeddings (dim={dim})\")\n",
    "faiss.write_index(index, \"rag_index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8789acc9-6ac7-44a1-afa6-3fd8ed83106c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Query RAG (blank to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query>  Explain what uber's next goal is?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– I think the next goal is to continue to grow the business. I think the next goal is to continue to grow the business. I think the next goal is to continue to grow the business. I think the next goal is to continue to grow the business. I think the next goal is to continue to grow the business. I think the next goal is to continue to grow the business. I think the next goal is to continue to grow the business. I think the next goal is to continue to grow the business. I think the next goal is to continue to grow the business. I think the next goal is to continue to grow the business. I think the next goal is to continue to grow the business. I think the next goal is to continue to grow the business. I think the next goal is to continue to grow the business. I think the next goal is to continue to grow the business. I think the next goal is to continue to grow the business. I think the next goal is to continue to grow the business. I think the next goal is to continue to grow the business. I think the next goal is to continue to grow the business. I think the next goal is to continue to grow the business. I think the next goal is to continue \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ” Query RAG (blank to quit)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     q = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQuery> \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.strip()\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m q:\n\u001b[32m     70\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1280\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1324\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "from llama_cpp import Llama\n",
    "\n",
    "\n",
    "index = faiss.read_index(\"rag_index.faiss\")\n",
    "with open(\"chunks.pkl\", \"rb\") as f:\n",
    "    chunks = pickle.load(f)   # List[str]\n",
    "\n",
    "\n",
    "SYSTEM = (\n",
    "    \"You are in charge of advising a finance company on information related to Uber. \"\n",
    "    \"Justify any claims with examples from the context provided. \"\n",
    "    \"Maintain a serious and professional tone.\"\n",
    ")\n",
    "\n",
    "EXAMPLE = \"\"\"\n",
    "Example:\n",
    "Context:\n",
    "Uberâ€™s rideshare revenue grew 20% last quarter.\n",
    "\n",
    "Question:\n",
    "What does this growth imply for Uberâ€™s position in the market?\n",
    "Answer:\n",
    "This 20% rideshare revenue increase underscores Uberâ€™s ability to capture market share from traditional taxis, \"\n",
    "which positions it as the dominant on-demand mobility provider. [source: snippet 0]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def retrieve(query: str, k: int = 5) -> list[str]:\n",
    "    resp = llm_emb.create_embedding(input=[query])\n",
    "    tok_emb = np.array(resp[\"data\"][0][\"embedding\"], dtype=np.float32)\n",
    "    \n",
    "    emb = tok_emb.mean(axis=0)\n",
    "    q_emb = emb.reshape(1, -1)  \n",
    "\n",
    "    _, I = index.search(q_emb, k)\n",
    "    return [chunks[i] for i in I[0]]\n",
    "\n",
    "\n",
    "def answer(query: str) -> str:\n",
    "    docs = retrieve(query, k=5)\n",
    "\n",
    "    numbered = [f\"[snippet {i}]\\n{txt}\" for i, txt in enumerate(docs)]\n",
    "    context = \"\\n\\n---\\n\\n\".join(numbered)\n",
    "\n",
    "    prompt = f\"\"\"{SYSTEM}\n",
    "\n",
    "{EXAMPLE}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    out = llm_chat(prompt, max_tokens=256, temperature=0.1)\n",
    "    return out[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸ” Query RAG (blank to quit)\")\n",
    "    while True:\n",
    "        q = input(\"Query> \").strip()\n",
    "        if not q:\n",
    "            break\n",
    "        print(\"ðŸ¤–\", answer(q), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ce6b7a-c3e5-4701-ba5e-d413eeb3146b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e542671c-c404-4c3a-903d-9cdc3a926af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4605b1ef-aa60-42ce-a33b-a9a26cf8f46b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5090cb-1cb1-473e-878e-cfc73cdaa44c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb2aa38-2263-40b2-a23b-d0626b28e8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Query RAG (blank to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query>  what is uber spending their money on?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Uber is spending their money on the following:\n",
      "\n",
      "1. Uber Eats (food delivery)\n",
      "2. Rideshare business in the US and abroad\n",
      "3. Self-driving cars\n",
      "4. Other miscellaneous projects like bike sharing, etc.\n",
      "\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query>  What possible risks are there if we merge with uber?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are a few potential risks that could arise if we merge with Uber. First, there is the risk of losing our unique identity and culture as an organization. If we were to combine forces with another company, it would be easy for us to lose sight of what makes us special and instead focus on becoming just like everyone else. Secondly, there's always a chance that something goes wrong during integration or implementation which could lead to cost overruns or delays in getting products out the door. Finally, if we were able to merge successfully then it would mean less competition within our industry - meaning fewer opportunities for\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "MAX_CTX           = 2048\n",
    "MAX_GEN           = 128\n",
    "RESERVED          = MAX_GEN + 50\n",
    "MAX_PROMPT_TOKENS = MAX_CTX - RESERVED\n",
    "\n",
    "SYSTEM = (\n",
    "    \"You are a concise, professional assistant advising a finance company about Uber. \"\n",
    "    \"Use only the provided context; justify claims with snippet citations.\"\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "# â”€â”€ MODEL INITS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "llm_emb = Llama(\n",
    "    model_path=\"path/to/model.gguf\",\n",
    "    gpu_layers=-1,       # all on GPU\n",
    "    n_ctx=MAX_CTX,\n",
    "    n_threads=8,\n",
    "    use_mmap=False,\n",
    "    use_mlock=True\n",
    ")\n",
    "llm_chat = Llama(\n",
    "    model_path=\"path/to/model.gguf\",\n",
    "    gpu_layers=-1,\n",
    "    n_ctx=MAX_CTX,\n",
    "    n_threads=8,\n",
    "    use_mmap=False,\n",
    "    use_mlock=True\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# â”€â”€ LOAD INDEX â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "index = faiss.read_index(\"rag_index.faiss\")\n",
    "with open(\"chunks.pkl\",\"rb\") as f:\n",
    "    chunks = pickle.load(f)\n",
    "\n",
    "# â”€â”€ RETRIEVAL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def retrieve(query: str, k: int = 3) -> list[str]:\n",
    "    resp    = llm_emb.create_embedding(input=[query])\n",
    "    tok_emb = np.array(resp[\"data\"][0][\"embedding\"], dtype=np.float32)\n",
    "    emb     = tok_emb.mean(axis=0).reshape(1, -1)   # (1,4096)\n",
    "    _, I    = index.search(emb, k)\n",
    "    return [chunks[i] for i in I[0]]\n",
    "\n",
    "# â”€â”€ PROMPT BUILDER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def build_prompt(query: str, docs: list[str]) -> str:\n",
    "    numbered = [f\"[snippet {i}]\\n{txt}\" for i, txt in enumerate(docs)]\n",
    "    while True:\n",
    "        context = \"\\n\\n---\\n\\n\".join(numbered)\n",
    "        prompt  = (\n",
    "            SYSTEM + \"\\n\\n\"\n",
    "            f\"Context:\\n{context}\\n\\n\"\n",
    "            f\"Question:\\n{query}\\n\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "        if len(prompt.split()) <= MAX_PROMPT_TOKENS or not numbered:\n",
    "            return prompt\n",
    "        numbered.pop()  # drop last snippet to shrink context\n",
    "\n",
    "# â”€â”€ ANSWERING (streaming) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def answer_stream(query: str):\n",
    "    docs   = retrieve(query, k=3)\n",
    "    prompt = build_prompt(query, docs)\n",
    "\n",
    "    # NOTE: no stop=[\"\\n\"] here â€” let the model actually write!\n",
    "    for token in llm_chat(\n",
    "        prompt,\n",
    "        max_tokens=MAX_GEN,\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        repeat_penalty=1.2,\n",
    "        stream=True\n",
    "    ):\n",
    "        print(token[\"choices\"][0][\"text\"], end=\"\", flush=True)\n",
    "    print()  # final newline\n",
    "\n",
    "# â”€â”€ REPL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸ” Query RAG (blank to quit)\")\n",
    "    while True:\n",
    "        q = input(\"Query> \").strip()\n",
    "        if not q:\n",
    "            break\n",
    "        answer_stream(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acb7ce6-8a7f-4b3a-93d5-5ecda1585b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "38b6a771-04b7-4b79-bec8-9f8e473a7c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I hope you are doing well. everybody. I'm doing well. I'm doing well. I'm doing well. I'm doing well. I'm doing well. I'm doing well. I'm doing well. I'm doing well. I'm doing well. I'm doing well. I'm doing well. I'm doing well. I'm doing well. I'm doing well. I'm doing well. I'm doing well. I'm doing well. I'm doing well. I'm doing well. I'm doing well\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chat(\"How are you doing?\", max_tokens=128, temperature=0.1)[\"choices\"][0][\"text\"].strip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
